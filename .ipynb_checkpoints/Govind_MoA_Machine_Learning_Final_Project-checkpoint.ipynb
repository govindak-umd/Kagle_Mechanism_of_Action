{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanisms of Action (MoA) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of the important terms used in the headings of the tables are presented here:\n",
    "    \n",
    "    g - : signifies gene expression data\n",
    "    c - : signifies cell expression data\n",
    "    cp_type : indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle)\n",
    "    NOTE: (samples with control perturbations don't have MoAs)\n",
    "    cp_time - treatment duration (24,48,72) Hours\n",
    "    cp_dose - Dosage - HIGH or LOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the multi label stratified k-fold \n",
    "# cross validator\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "# Initial random imports\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Importing matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device_code = 'cuda'\n",
    "else:\n",
    "    device_code = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the seed, so that every time the seed is started from the same number\n",
    "\n",
    "def set_seed_characteristics(seed=55):\n",
    "    # Setting a random seed value\n",
    "    \n",
    "    random.seed(seed)\n",
    "    \n",
    "    # for guaranteering the reproducability of numbers by setting seed for NumPy\n",
    "    \n",
    "    np.random.seed(seed) \n",
    "    \n",
    "    # for setting the seed for cuda or cpu\n",
    "    \n",
    "    torch.manual_seed(seed) \n",
    "\n",
    "    # To ensure that Pytorch doesnt just switch to the fastest possible algorithm but \n",
    "    # ensures that it selects a deterministic algorithm\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input/train_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-072177f21c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input/train_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Reading the head rows and columns of train features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining_features_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_targets_scored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input/train_targets_scored.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfenv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfenv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfenv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfenv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfenv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input/train_features.csv'"
     ]
    }
   ],
   "source": [
    "training_features = pd.read_csv('input/train_features.csv')\n",
    "# Reading the head rows and columns of train features\n",
    "training_features_head = training_features.head()\n",
    "\n",
    "training_targets_scored = pd.read_csv('input/train_targets_scored.csv')\n",
    "# Reading the head rows and columns of train targets scored\n",
    "training_targets_scored_head = training_targets_scored.head()\n",
    "\n",
    "testing_features = pd.read_csv('input/test_features.csv')\n",
    "# Reading the head rows and columns of train targets non-scored\n",
    "testing_features_head = testing_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the head - training features \n",
    "training_features_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the head - train targets scored \n",
    "training_targets_scored_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the head - test features\n",
    "testing_features_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset classes, training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch data loader implementation of MoA dataset\n",
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        train_tensor_dictionary = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return train_tensor_dictionary\n",
    "\n",
    "# Pytorch data loader implementation of test dataset\n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        test_tensor_dictionary = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return test_tensor_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch model for the MoA determination\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    # Instantiaing all the models before utilizing\n",
    "    # them later in the forward function.\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        \n",
    "        # super keyword used to access data from the parent\n",
    "        # pytorch.nn.Module class\n",
    "        super(Model, self).__init__()\n",
    "        # Applying batch normalization. This is done to standardize\n",
    "        # the input for each mini batches and will help reduce the\n",
    "        # number of epochs for which the training is done. This limits\n",
    "        # the covariate shift (this is the value by which the hidden\n",
    "        # layer values shift around) and allows to learn from a more \n",
    "        # stable set of data. Sometimes, it also allows for a\n",
    "        # higher learning rate.This is also used for regularization\n",
    "        # and helps reduce over fitting. Generally, if batch \n",
    "        # normalization is used, you can use a smaller dropout,\n",
    "        # which in turn means that lesser layers can be lost \n",
    "        # in every step.\n",
    "        self.batch_normalization_1 = nn.BatchNorm1d(num_features)        \n",
    "        # For regularization purposes the dropout is set\n",
    "        # This is done by setting a probablity. Random \n",
    "        # neural networks are picked at a probablity, say p\n",
    "        # or dropped at a probablity of 1-p. This is essential \n",
    "        # to prevent overfitiing of the model and also reduces\n",
    "        # the computation time. A fully connected neural network, if\n",
    "        # run without dropout will start forming dependancies between\n",
    "        # each other and this can lead to over-fitting.\n",
    "        self.dropoutlayer_1 = nn.Dropout(0.2)\n",
    "        # nn.utils.weight_norm : This is weight normalization. Usually,\n",
    "        #                        faster than batch normalization\n",
    "        # nn.Linear : Applying linear transform to the incoming data\n",
    "        #             and creates a single layer feed forward network.\n",
    "        # input size : num_features\n",
    "        # output size : hidden_size\n",
    "        self.denselayer_1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_normalization_2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropoutlayer_2 = nn.Dropout(0.2)\n",
    "        # input size : hidden_size\n",
    "        # output size : hidden_size\n",
    "        self.denselayer_2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_normalization_3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropoutlayer_3 = nn.Dropout(0.1)\n",
    "        # input size : hidden_size\n",
    "        # output size : hidden_size\n",
    "        self.denselayer_3 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_normalization_4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropoutlayer_4 = nn.Dropout(0.1)\n",
    "        # input size : hidden_size\n",
    "        # output size : hidden_size\n",
    "        self.denselayer_4 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        \n",
    "        self.batch_normalization_5 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropoutlayer_5 = nn.Dropout(0.1)\n",
    "        # input size : hidden_size\n",
    "        # output size : num_targets\n",
    "        self.denselayer_5 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    # The forward function basically defines the model\n",
    "    def forward(self, forward_x):\n",
    "        \n",
    "        forward_x = self.batch_normalization_1(forward_x)\n",
    "        forward_x = self.dropoutlayer_1(forward_x)\n",
    "        forward_x = F.relu(self.denselayer_1(forward_x))\n",
    "        \n",
    "        forward_x = self.batch_normalization_2(forward_x)\n",
    "        forward_x = self.dropoutlayer_2(forward_x)\n",
    "        forward_x = F.relu(self.denselayer_2(forward_x))\n",
    "        \n",
    "        forward_x = self.batch_normalization_3(forward_x)\n",
    "        forward_x = self.dropoutlayer_3(forward_x)\n",
    "        forward_x = self.denselayer_3(forward_x)\n",
    "        \n",
    "        forward_x = self.batch_normalization_4(forward_x)\n",
    "        forward_x = self.dropoutlayer_4(forward_x)\n",
    "        forward_x = self.denselayer_4(forward_x)\n",
    "\n",
    "        forward_x = self.batch_normalization_5(forward_x)\n",
    "        forward_x = self.dropoutlayer_5(forward_x)\n",
    "        forward_x = self.denselayer_5(forward_x)\n",
    "        \n",
    "        return forward_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def trainingFunction(model, optimizer, scheduler, lossFunction, trainloader, device_code):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for training_data in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = training_data['x'].to(device_code), training_data['y'].to(device_code)\n",
    "        outputs = model(inputs)\n",
    "        loss = lossFunction(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        training_loss += loss.item()    \n",
    "    training_loss /= len(trainloader) \n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate the model\n",
    "def validationFunction(model, lossFunction, validationloader, device_code):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    validation_predictions = []   \n",
    "    for validation_data in validationloader:\n",
    "        inputs, targets = validation_data['x'].to(device_code), validation_data['y'].to(device_code)\n",
    "        outputs = model(inputs)\n",
    "        loss = lossFunction(outputs, targets)\n",
    "        validation_loss += loss.item()\n",
    "        validation_predictions.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "    validation_loss /= len(validationloader)\n",
    "    validation_predictions = np.concatenate(validation_predictions)\n",
    "    return validation_loss, validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the inference function\n",
    "def inferenceFunction(model, inferenceloader, device_code):\n",
    "    model.eval()\n",
    "    inferences = [] \n",
    "    for data in inferenceloader:\n",
    "        inputs = data['x'].to(device_code)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        inferences.append(outputs.sigmoid().detach().cpu().numpy())   \n",
    "    inferences = np.concatenate(inferences)  \n",
    "    return inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dummy inserts to the cp_time and cp_dose columns\n",
    "# Usually done to categorical variables\n",
    "def addDummies(data):\n",
    "    dummy_data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return dummy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed_characteristics(seed=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating out the Gene expression Column and Cell Viability Column\n",
    "\n",
    "gene_expression = [g for g in training_features.columns if g.startswith('g-')]\n",
    "cell_viability = [c for c in training_features.columns if c.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our dimensions are really high, we can resort to \n",
    "# using PCA for dimensionality reduction, but is still able \n",
    "# to capture the characteristics of the data.\n",
    "\n",
    "# Now, this can be done by choosing a random dimension, and \n",
    "# having the same random state as before. By doing this\n",
    "# we observe that we do not encounter\n",
    "# any 'nan' errors during training.\n",
    "\n",
    "# Doing PCA for the Gene expression data\n",
    "\n",
    "# can choose any random number here\n",
    "random_pca_dimension_genes = 20\n",
    "\n",
    "# Concatenating the training and test set\n",
    "data = pd.concat([pd.DataFrame(training_features[gene_expression]), pd.DataFrame(testing_features[gene_expression])])\n",
    "\n",
    "# Performing PCA and converting to a random_pca_dimension_genes number of columns\n",
    "pca_genes = PCA(n_components = random_pca_dimension_genes, random_state=55)\n",
    "\n",
    "# Fitting the PCA transform\n",
    "data_pca = pca_genes.fit_transform(data[gene_expression])\n",
    "\n",
    "# Splitting the training and test columns\n",
    "train_pca_genes = data_pca[:training_features.shape[0]] \n",
    "test_pca_genes = data_pca[-testing_features.shape[0]:]\n",
    "\n",
    "# Converting training and testing  into Pandas data frame shape\n",
    "train_pca_genes = pd.DataFrame(train_pca_genes, columns=[f'pca_G-{i}' for i in range(random_pca_dimension_genes)])\n",
    "test_pca_genes = pd.DataFrame(test_pca_genes, columns=[f'pca_G-{i}' for i in range(random_pca_dimension_genes)])\n",
    "\n",
    "# Concatenating these back to the original features\n",
    "training_features = pd.concat((training_features, train_pca_genes), axis=1)\n",
    "testing_features = pd.concat((testing_features, test_pca_genes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing PCA for the Cell Viability Data\n",
    "\n",
    "# can choose any random number here\n",
    "random_pca_dimension_cells = 32\n",
    "\n",
    "# Concatenating the training and test set\n",
    "data = pd.concat([pd.DataFrame(training_features[cell_viability]), pd.DataFrame(testing_features[cell_viability])])\n",
    "\n",
    "# Performing PCA and converting to a random_pca_dimension_cells number of columns\n",
    "pca_cells = PCA(n_components = random_pca_dimension_cells, random_state=55)\n",
    "\n",
    "# Fitting the PCA transform\n",
    "data_pca = pca_cells.fit_transform(data[cell_viability])\n",
    "\n",
    "# Splitting the training and test columns\n",
    "train_pca_cells = data_pca[:training_features.shape[0]]\n",
    "test_pca_cells = data_pca[-testing_features.shape[0]:]\n",
    "\n",
    "# Converting training and testing  into Pandas data frame shape\n",
    "train_pca_cells = pd.DataFrame(train_pca_cells, columns=[f'pca_C-{i}' for i in range(random_pca_dimension_cells)])\n",
    "test_pca_cells = pd.DataFrame(test_pca_cells, columns=[f'pca_C-{i}' for i in range(random_pca_dimension_cells)])\n",
    "\n",
    "# Concatenating these back to the original features\n",
    "training_features = pd.concat((training_features, train_pca_cells), axis=1)\n",
    "testing_features = pd.concat((testing_features, test_pca_cells), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a desired threshold to calculate the VarianceThreshold.\n",
    "# As per the math all the Features with a training-set variance \n",
    "# lower than this threshold will be removed.\n",
    "variancethreshold = VarianceThreshold(threshold=0.7)\n",
    "\n",
    "# Combining training and test features to create a single dataset\n",
    "combined_data = training_features.append(testing_features)\n",
    "\n",
    "# Fits to the data, before transforming it\n",
    "combined_data_transformed = variancethreshold.fit_transform(combined_data.iloc[:, 4:])\n",
    "\n",
    "# Extracting the training and the testing data out of the\n",
    "# transformed data\n",
    "training_features_transformed = combined_data_transformed[ : training_features.shape[0]]\n",
    "testing_features_transformed = combined_data_transformed[-testing_features.shape[0] : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the training features in a suitable \n",
    "# pandas dataset format and numbering the columns\n",
    "# after the labels of 'sig_id', 'cp_type', 'cp_time', 'cp_dose'.\n",
    "training_features = pd.DataFrame(training_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4), columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "training_features = pd.concat([training_features, pd.DataFrame(training_features_transformed)], axis=1)\n",
    "\n",
    "# Extracting the testing features in a suitable \n",
    "# pandas dataset format and numbering the columns\n",
    "# after the labels of 'sig_id', 'cp_type', 'cp_time', 'cp_dose'.\n",
    "\n",
    "testing_features = pd.DataFrame(testing_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4), columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "testing_features = pd.concat([testing_features, pd.DataFrame(testing_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the columns\n",
    "\n",
    "train = training_features.merge(training_targets_scored, on='sig_id')\n",
    "\n",
    "# Removing rows with cp_type as ctl_vehicle \n",
    "# since control perturbations have no MoAs\n",
    "# We are also manually setting the drop type as \n",
    "# true because we do not want to include them back \n",
    "# as a new column.\n",
    "\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "# Naturally, we have to get rid of them from the test dataset \n",
    "# as well\n",
    "\n",
    "test = testing_features[testing_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the columns of the drugs that are sold from\n",
    "# the train pandas dataframe\n",
    "\n",
    "target = train[training_targets_scored.columns]\n",
    "\n",
    "# Now that the ctl_vehicle drugs have been removed, we do not need\n",
    "# cp_type. So we can go ahead and remove that columns as well.\n",
    "\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "# extracting the columns in the targets \n",
    "\n",
    "target_columns = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel stratified K Fold import causes a small warning and we do not want\n",
    "# to show that in the notebook.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "folds = train.copy()\n",
    "number_of_folds = 3\n",
    "\n",
    "# creating a 3 fold multilabel stratified K Fold\n",
    "multilabel_k_fold = MultilabelStratifiedKFold(n_splits = number_of_folds)\n",
    "\n",
    "# Standard k fold splitting. Here we are splitting into number_of_folds folds\n",
    "\n",
    "for fol, (train_folds, validation_folds) in enumerate(multilabel_k_fold.split(X=train, y=target)):\n",
    "    folds.loc[validation_folds, 'kfold'] = int(fol)\n",
    "    \n",
    "folds['kfold'] = folds['kfold'].astype(int)\n",
    "\n",
    "# Isolating out the feature columns. This is done by first \n",
    "# Isolating the columns that are not present in the target\n",
    "# followed by extracting the columns except the sig_id and \n",
    "# kfold.\n",
    "\n",
    "feature_columns = [c for c in addDummies(folds).columns if c not in target_columns]\n",
    "feature_columns = [c for c in feature_columns if c not in ['kfold','sig_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring the HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "max_epochs = 16\n",
    "# When training neural networks, it is common to use \n",
    "# weight decay where after each update, the weights \n",
    "# are multiplied by a factor slightly less than 1\n",
    "weight_decay = 1e-5\n",
    "# deciding the initial learning rate\n",
    "# It controls how quickly or slowly a neural\n",
    "# network model can learn a model or a problem.\n",
    "lr = 1e-3\n",
    "# Boolean to decide on stopping early when the \n",
    "# validation_loss > best_loss\n",
    "bool_early_stop = True\n",
    "# steps to execute before early stopping\n",
    "steps_early_stopping= 10\n",
    "# number of features corresponding to the columns in the\n",
    "# targets\n",
    "num_features=len(feature_columns)\n",
    "# number of targets corresponding to the columns in the\n",
    "# features\n",
    "num_targets=len(target_columns)\n",
    "# in between neural netwrok size\n",
    "hidden_size=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring the training functions and performing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot validation loss\n",
    "valid_loss_list = []\n",
    "# to plot the training loss\n",
    "train_loss_list = []\n",
    "# to plot the best recorded loss\n",
    "best_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    # declaring the list as global to plot validation loss\n",
    "    global valid_loss_list\n",
    "    # declaring the training loss list as global to plot\n",
    "    # the training loss\n",
    "    global train_loss_list\n",
    "    # declaring the best loss list as global to plot the\n",
    "    # best losses recorded\n",
    "    global best_loss_list\n",
    "    \n",
    "    # setting the seed to start from the same number as \n",
    "    # explained previously\n",
    "    set_seed_characteristics(seed)\n",
    "    \n",
    "    # adding dummy variables to the training set\n",
    "    train = addDummies(folds)\n",
    "\n",
    "    # extracting the validating rows numbers for the\n",
    "    # respective k fold values\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    # Dropping all the rows from the training set\n",
    "    # that does not belong to this kth fold\n",
    "    train_necessary_rows = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    # Dropping all the rows from the valiadtion set\n",
    "    # that does not belong to this kth fold\n",
    "    valid_necessary_rows = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    # splitting the x and y values for training set\n",
    "    train_features, train_targets  = train_necessary_rows[feature_columns].values, train_necessary_rows[target_columns].values\n",
    "    # splitting the x and y values for test set\n",
    "    validation_features, validation_targets =  valid_necessary_rows[feature_columns].values, valid_necessary_rows[target_columns].values\n",
    "    \n",
    "    # Converting the training data to standard pytorch \n",
    "    # dataset class format\n",
    "    train_dataset = MoADataset(train_features, train_targets)\n",
    "    \n",
    "    # Converting the validation data to standard pytorch \n",
    "    # dataset class format\n",
    "    valid_dataset = MoADataset(validation_features, validation_targets)\n",
    "    \n",
    "    # calling the pytorch data loading utility for the\n",
    "    # training set\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # calling the pytorch data loading utility for the\n",
    "    # validation set  \n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Declaring the model and can be tuned here\n",
    "    # using the hyper parameters\n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    # moving the model to GPU if available,\n",
    "    # else will run it on CPU itself\n",
    "    model.to(device_code)\n",
    "    \n",
    "    # A standard optimizer. Adam optimizer is widely used\n",
    "    # because it combines the advantages of the Adaptive gradient\n",
    "    # algorithm and the root mean square propogation. Basically, it does\n",
    "    # not stick to one learning rate and adapts it to the problem. \n",
    "    # It is widely known to offer good results really fast.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # We use a learning rate scheduler to converge to the lowest\n",
    "    # loss faster. This is also seen to provide higher accuracy.\n",
    "    # This can be tuned.\n",
    "    # Some of the optimizers I tried here are\n",
    "    # optim.lr_scheduler.OneCycleLR\n",
    "    # optim.lr_scheduler.StepLR\n",
    "    \n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.05, div_factor=1.5e3, \n",
    "                                              max_lr=1e-2, epochs=max_epochs, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    # after research I saw that the Binary cross\n",
    "    # entropy loss with sigmoid layer works well\n",
    "    lossFunction = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # stops when the error starts increaseing. Setting the counter\n",
    "    # to track this\n",
    "    steps_before_early_stop = 0\n",
    "    # general out of fold array shape\n",
    "    out_of_fold = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    # declaring a very high value as an \n",
    "    # initial loss for each kth fold\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # looping through the epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # training the model\n",
    "        training_loss = trainingFunction(model, optimizer,scheduler, lossFunction, trainloader, device_code)\n",
    "        print('epoch : ',epoch,'>> training_loss : ',training_loss)\n",
    "        train_loss_list.append(training_loss)\n",
    "        validation_loss, validation_predictions = validationFunction(model, lossFunction, validloader, device_code)\n",
    "        print('epoch : ',epoch,'>> validation : ',validation_loss)\n",
    "        valid_loss_list.append(validation_loss)\n",
    "        \n",
    "        # checking if the loss is decreasing\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            best_loss_list.append(best_loss)\n",
    "            # Updating the out of fold predictions\n",
    "            out_of_fold[val_idx] = validation_predictions\n",
    "            # saving the model and data for this kth fold\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        # Handling the increasing loss by calling \n",
    "        # early stopping\n",
    "        elif(bool_early_stop == True):\n",
    "            \n",
    "            # breaks out of the loop when this happens\n",
    "            steps_before_early_stop += 1\n",
    "            if (steps_before_early_stop >= steps_early_stopping):\n",
    "                break\n",
    "    # adding dummy variables to the test set\n",
    "    test_ = addDummies(test)       \n",
    "    \n",
    "    # extracting the x_test\n",
    "    x_test = test_[feature_columns].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(num_features=num_features,num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    # uploading the saved data for this kth fold\n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    # again uploading the model to GPU, if available\n",
    "    model.to(device_code)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    # evaluates the model\n",
    "    predictions = inferenceFunction(model, testloader, device_code)\n",
    "    \n",
    "    return out_of_fold, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeKFold(number_of_folds, seed):\n",
    "    # standard size for the out of fold predictions\n",
    "    out_of_fold = np.zeros((len(train), len(target_columns)))\n",
    "    # same size for all of the predictions\n",
    "    predictions = np.zeros((len(test), len(target_columns)))\n",
    "    \n",
    "    for each_k_fold in range(number_of_folds):\n",
    "        print('Fold Number : ', each_k_fold)\n",
    "        out_of_fold_, pred_ = run_training(each_k_fold, seed)\n",
    "        \n",
    "        # adding all the predictions\n",
    "        predictions += pred_ / number_of_folds\n",
    "        # adding all the out of fold predictions\n",
    "        out_of_fold += out_of_fold_\n",
    "        print(\"------------------------\")\n",
    "        \n",
    "    k_th_prediction = predictions\n",
    "    return out_of_fold, k_th_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a standard seed number\n",
    "SEED = [55]\n",
    "# general out of fold array shape\n",
    "out_of_fold = np.zeros((len(train), len(target_columns)))\n",
    "# general predictions array shape\n",
    "predictions = np.zeros((len(test), len(target_columns)))\n",
    "\n",
    "# for seed in SEED:\n",
    "out_of_fold_, predictions_ = executeKFold(number_of_folds, SEED[0])\n",
    "out_of_fold += out_of_fold_ / len(SEED)\n",
    "predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_columns] = out_of_fold\n",
    "test[target_columns] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the logarithmic loss function applied to each drug-MoA annotation pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = training_targets_scored.drop(columns=target_columns).merge(train[['sig_id']+target_columns], on='sig_id', how='left').fillna(0)\n",
    "# True target values\n",
    "true_target = training_targets_scored[target_columns].values\n",
    "# Predicted target values\n",
    "predicted_target = validation_df[target_columns].values\n",
    "cross_validation_score = 0\n",
    "\n",
    "# Now we can calculate the cross entropy loss\n",
    "\n",
    "for i in range(len(target_columns)):\n",
    "    cross_validation_score_target = log_loss(true_target[:, i], predicted_target[:, i])\n",
    "    cross_validation_score += cross_validation_score_target / target.shape[1]  \n",
    "\n",
    "print(\" The Cross Validation loss is :>> \", cross_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Validation loss for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss_list)\n",
    "plt.title('Validation Loss')\n",
    "plt.savefig('valid_loss_list.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Training loss for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.title('Training loss')\n",
    "plt.savefig('train_loss_list.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Best recorded loss for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best_loss_list)\n",
    "plt.title('Best Recorded loss')\n",
    "plt.savefig('best_loss_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
